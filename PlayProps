package com.example;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Profile;
import org.springframework.expression.common.LiteralExpression;
import org.springframework.integration.annotation.IntegrationComponentScan;
import org.springframework.integration.config.EnableIntegration;
import org.springframework.integration.dsl.IntegrationFlow;
import org.springframework.integration.dsl.IntegrationFlows;
import org.springframework.integration.dsl.MessageChannels;
import org.springframework.integration.kafka.outbound.KafkaProducerMessageHandler;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.kafka.support.KafkaHeaders;
import org.springframework.kafka.support.serializer.JsonSerializer;
import org.springframework.messaging.Message;
import org.springframework.messaging.MessageChannel;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;
import org.springframework.util.StringUtils;

import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.RejectedExecutionHandler;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.atomic.AtomicLong;

import static org.apache.kafka.clients.producer.ProducerConfig.*;

@Slf4j
@Configuration
@EnableIntegration
@IntegrationComponentScan
@RequiredArgsConstructor
@Profile("kafka")
public class OutboundFlowKafkaConfig {

    private static final String APP_NAME = "MyApp-otel-collector";

    private final KafkaConfig kafkaConfig;

    // Counters you can later export via Micrometer
    private final AtomicLong droppedOnOverload = new AtomicLong(0);
    private final AtomicLong droppedOnKafkaFailure = new AtomicLong(0);

    /**
     * Bulkhead executor: isolates Kafka publishing from the rest of the app.
     *
     * IMPORTANT: We choose a "drop on overload" policy (syslog-friendly).
     * When Kafka is slow/unavailable, we do NOT want to block upstream threads indefinitely.
     */
    @Bean
    public ThreadPoolTaskExecutor kafkaOutboundExecutor() {
        ThreadPoolTaskExecutor ex = new ThreadPoolTaskExecutor();
        ex.setCorePoolSize(16);
        ex.setMaxPoolSize(64);

        // Tune based on memory + expected burst size.
        // If this queue fills, we DROP (see RejectedExecutionHandler below).
        ex.setQueueCapacity(10_000);

        ex.setThreadNamePrefix("kafka-outbound-");

        // Drop-on-overload policy with logging + counter.
        ex.setRejectedExecutionHandler(dropOnReject("kafkaOutboundExecutor"));

        ex.initialize();
        return ex;
    }

    /**
     * Error handling executor: must never block producer threads.
     * Keep error handling fast (log/metrics/alerts), then return => drop.
     */
    @Bean
    public ThreadPoolTaskExecutor kafkaErrorExecutor() {
        ThreadPoolTaskExecutor ex = new ThreadPoolTaskExecutor();
        ex.setCorePoolSize(2);
        ex.setMaxPoolSize(8);
        ex.setQueueCapacity(50_000);
        ex.setThreadNamePrefix("kafka-error-");
        ex.setRejectedExecutionHandler(new ThreadPoolExecutor.DiscardPolicy());
        ex.initialize();
        return ex;
    }

    /**
     * Producer error channel MUST be non-blocking. ExecutorChannel is best here.
     */
    @Bean
    public MessageChannel producerErrorChannel(@Qualifier("kafkaErrorExecutor") ThreadPoolTaskExecutor errorExecutor) {
        return MessageChannels.executor(errorExecutor).getObject();
    }

    /**
     * Main outbound flow:
     * - Offloads work to a dedicated executor (bulkhead).
     * - NO SI retry.
     * - Kafka send is async (sync=false).
     * - Failures go to producerErrorChannel where we log/metric and DROP.
     */
    @Bean
    public IntegrationFlow kafkaIntegrationFlow(
            @Qualifier("producerChannel") MessageChannel producerChannel,
            @Qualifier("kafkaOutboundExecutor") ThreadPoolTaskExecutor kafkaOutboundExecutor,
            KafkaProducerMessageHandler<String, MyAppSyslogEvent> kafkaMessageHandler
    ) {
        return IntegrationFlows.from(producerChannel)
                .channel(MessageChannels.executor(kafkaOutboundExecutor))
                .handle(kafkaMessageHandler) // <-- no SI retry advice
                .get();
    }

    /**
     * Drop-on-failure handler:
     * Anything arriving here failed to publish to Kafka (after Kafka client's own retries/timeouts).
     * We LOG + increment a counter, and then STOP (no resend, no DLQ).
     */
    @Bean
    public IntegrationFlow producerErrorHandlingFlow(@Qualifier("producerErrorChannel") MessageChannel errorChannel) {
        return IntegrationFlows.from(errorChannel)
                .handle(m -> {
                    droppedOnKafkaFailure.incrementAndGet();

                    // Keep logging lightweight: log topic/key + exception summary (avoid huge payload logging)
                    Object topic = m.getHeaders().get(KafkaHeaders.TOPIC);
                    Object key = m.getHeaders().get(KafkaHeaders.MESSAGE_KEY);

                    // For KafkaProducerMessageHandler, failures are often wrapped; we log headers + payload type.
                    log.warn("Kafka publish failed => DROP. topic={}, key={}, headersKeys={}, payloadType={}",
                            topic, key, m.getHeaders().keySet(),
                            m.getPayload() != null ? m.getPayload().getClass().getName() : "null");
                })
                .get();
    }

    /**
     * Producer configs tuned for: syslog, huge volume, resilience.
     * Policy: do not block indefinitely; retry a bit for transient issues; then fail.
     */
    @Bean
    public Map<String, Object> producerConfigs() {
        Map<String, Object> props = new HashMap<>();

        props.put(BOOTSTRAP_SERVERS_CONFIG, kafkaConfig.getBrokers());
        props.put(KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);

        // You want acks=1
        props.put(ACKS_CONFIG, "1");

        // Idempotence requires acks=all, so keep it OFF for your stated requirement.
        props.put(ENABLE_IDEMPOTENCE_CONFIG, false);

        // Fail-fast / no-wedge guards
        props.put(REQUEST_TIMEOUT_MS_CONFIG, 15_000);  // per request
        props.put(DELIVERY_TIMEOUT_MS_CONFIG, 45_000); // total time allowed for a record
        props.put(MAX_BLOCK_MS_CONFIG, 5_000);         // cap blocking if metadata/buffer is unavailable

        // Finite retries: avoid "2147483646 attempts left"
        props.put(RETRIES_CONFIG, 5);
        props.put(RETRY_BACKOFF_MS_CONFIG, 250);

        // Throughput
        props.put(LINGER_MS_CONFIG, 10);
        props.put(BATCH_SIZE_CONFIG, 64 * 1024);
        props.put(BUFFER_MEMORY_CONFIG, 256L * 1024 * 1024); // 256MB (tune)
        props.put(COMPRESSION_TYPE_CONFIG, "lz4");

        // Usually good for throughput; ordering across retries is less strict since idempotence is off.
        props.put(MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);

        props.putAll(kafkaConfig.sslConfig());
        return props;
    }

    @Bean
    public ProducerFactory<String, MyAppSyslogEvent> producerFactory() {
        return new DefaultKafkaProducerFactory<>(producerConfigs());
    }

    @Bean
    public KafkaTemplate<String, MyAppSyslogEvent> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }

    /**
     * Async Kafka handler (sync=false). Failures go to producerErrorChannel and get DROPPED there.
     */
    @Bean
    public KafkaProducerMessageHandler<String, MyAppSyslogEvent> kafkaMessageHandler(
            @Qualifier("producerErrorChannel") MessageChannel producerErrorChannel
    ) {
        KafkaProducerMessageHandler<String, MyAppSyslogEvent> handler =
                new KafkaProducerMessageHandler<>(kafkaTemplate());

        handler.setTopicExpression(new LiteralExpression(kafkaConfig.getTopic()));
        handler.setMessageKeyExpression(new LiteralExpression(APP_NAME));

        // NOTE: setSendFailureChannel is the important one for async send failures.
        handler.setSendFailureChannel(producerErrorChannel);

        // Resilience choice: async so Kafka issues don't block the integration thread.
        handler.setSync(false);

        // Keep this small; it is NOT the Kafka request timeout, but it avoids long blocking on channel sends.
        handler.setSendTimeout(1_000);

        return handler;
    }

    // ---- helpers ----

    private RejectedExecutionHandler dropOnReject(String executorName) {
        return (r, executor) -> {
            long n = droppedOnOverload.incrementAndGet();
            if (n == 1 || (n % 10_000) == 0) {
                log.warn("{} overloaded => DROP. droppedOnOverload={}", executorName, n);
            }
            // drop: do nothing (discard task)
        };
    }
}
